%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University/School Laboratory Report
% LaTeX Template
% Version 3.1 (25/3/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Linux and Unix Users Group at Virginia Tech Wiki
% (https://vtluug.org/wiki/Example_LaTeX_chem_lab_report)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

%\usepackage[version=3]{mhchem} % Package for chemical equation typesetting
%\usepackage{siunitx} % Provides the \SI{}{} and \si{} command for typesetting SI units
\usepackage{graphicx} % Required for the inclusion of images
\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements
\usepackage[ruled,vlined]{algorithm2e}

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%\usepackage{xcolor}
%\newcommand{\gray}[1]{\textcolor{gray}{#1}}

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)
\input{defs.tex}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
%\usepackage{times} % Uncomment to use the Times New Roman font

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Russian Roulette CG} % Title

\author{Luhuan Wu} % Author name

\date{\today} % Date for the report

\begin{document}

\maketitle % Insert the title, author and date


% If you wish to include an abstract, uncomment the lines below
% \begin{abstract}
% Abstract text
% \end{abstract}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Krylov GPs}

Th objective is
\begin{align}\label{eq:objective}
 L(\theta | X, \btheta) &= -\log p(\by| \bX, \btheta) \propto  \log | \hat{\bK}_{\bX \bX} | + \by^T \hat{\bK}_{\bX\bX}^{-1} \by.
\end{align}
The derivative of Eq~\ref{eq:objective} is given by:
\begin{align}
  \frac{\partial -\log p(\by | \bX, \btheta)}{\partial \theta} &
   \propto \textrm{Tr}\left( \hat{\bK}_{\bX \bX}^{-1} \frac{\partial \hat{\bK}_{\bX\bX}}{\partial \theta}\right)
   - \by^T \hat{\bK}_{\bX\bX}^{-1} \frac{\partial \hat{\bK}_{\bX\bX}}{\partial \btheta} \hat{\bK}_{\bX\bX}^{-1} \by
\end{align}

We cal Algorithm~\ref{alg:mbcg} which takes $[\by \quad \bz_1 \quad \cdots \quad \bz_t]$ as inputs and outputs
\begin{align}
  [\bu_0 \quad \bu_1 \quad \cdots \quad \bu_t] &= \hat{\bK}_{\bX \bX}^{-1} [\by \quad \bz_1 \quad \cdots \quad \bz_t] \textrm{ and } \tilde{T}_1, \cdots, \tilde{T}_t
\end{align}
where $\tilde{T}_1, \cdots, \tilde{T}_t$ are partial Lanczos tridiagonalizatios of $\hat{\bK}_{\bX\bX}$ with respect to the vectors $\bz_1, \cdots, \bz_t$.


\begin{itemize}
  \item Computing $\by^T \hat{\bK}_{\bX\bX}^{-1} \by$: use dircte output from mBCG.
  \item Computing $\log | \hat{\bK}_{\bX \bX} |$: use stochastic trace estimation + Lanczos quadrature.
  \begin{align}
    \log | \hat{\bK}_{\bX \bX} | &= \textrm{Tr} (\log \hat{\bK}_{\bX \bX})\\
    & = \mathbb{E}\left[ \bz^T (\log \hat{\bK}_{\bX \bX}) \bz \right] \\
    & \approx \frac{1}{t} \sum_{i=1}^t \bz_i^T (\log \hat{\bK}_{\bX \bX}) \bz_i \\
    & \approx \frac{1}{t} \sum_{i=1}^t \left[ (\bV_i \be_1)^T (\log \bLambda_i ) (\bV_i \be_1) \right]
  \end{align}
  where $\tilde{\bT}_{i} =\bV_i \bLambda_i \bV_i^T$ is the eigenvalue decomposition.
  \item Computing $\textrm{Tr}\left( \hat{\bK}_{\bX \bX}^{-1} \frac{\partial \hat{\bK}_{\bX\bX}}{\partial \theta}\right)$: use stochastic trace estimation.
  \begin{align}
    \textrm{Tr}\left( \hat{\bK}_{\bX \bX}^{-1} \frac{\partial \hat{\bK}_{\bX\bX}}{\partial \theta}\right)
    &= \mathbb{E} \left[ \bz^T \hat{\bK}_{\bX \bX}^{-1} \frac{\partial \hat{\bK}_{\bX\bX}}{\partial \theta} \bz \right] \\
    &= \frac{1}{t} \sum_{i=1}^t \left(\bz_i^T \hat{\bK}_{\bX\bX}^{-1} \right) \left( \frac{\partial \hat{\bK}_{\bX \bX}}{\partial \btheta} \bz_i \right)
  \end{align}


\end{itemize}

In summary, the objective is estimated by
\begin{align}
  L(\theta | \bX, \btheta) & \approx \frac{1}{t} \sum_{i=1}^t \bz_i^T \left(\log \hat{\bK}_{\bX \bX} \right) \bz_i + \by^T \hat{\bK}_{\bX \bX}^{-1} \by,
\end{align}
and the derivative is estimated by
\begin{align}
  \frac{\partial L(\theta | \bX, \btheta)}{\partial \theta}
  &\approx \frac{1}{t} \sum_{i=1}^t \left( \bz_i^T \hat{\bK}_{\bX \bX}^{-1} \right)
  \left( \frac{\partial \hat{\bK}_{\bX\bX}}{\partial \btheta} \bz_i  \right)
  - \by^T \hat{\bK}_{\bX \bX}^{-1}  \frac{\partial \hat{\bK}_{\bX \bX}}{\partial \btheta} \hat{\bK}_{\bX \bX}^{-1} \by.
\end{align}


\begin{algorithm}[H]
\SetAlgoLined
\KwInput{$mmm_A()$ -- function for matrix-matrix multiplication with $A$ \\
        $B$ -- $n\times t$ matrix to solve against \\
        $\hat{P}^{-1}$ -- func. for preconditioner\\}
\KwOutput{$A^{-1}B$, $\tilde{T}_1, \cdots, \tilde{T}_t$}
 %initialization
 $U_0 \leftarrow \mathbf{0}$ \gray{// Current solutions} \\
 $R_0 \leftarrow B - mmm_A(U_0)$ \gray{// Current residuals} \\
 $Z_0 \leftarrow P^{-1}(R_0)$ \gray{// Preconditioned residuals} \\
 $D_0 \leftarrow Z_0$ \gray{// Search directions for next solutions} \\
 $\tilde{T}_1, \cdots, \tilde{T}_t \leftarrow 0$ \gray{// Tridiag matrices} \\
\For{$j \leftarrow 0$ \KwTo $t$}{
$V_j \leftarrow mmm_A (D_{j-1})$ \\
$\balpha \leftarrow (R_{j-1} \circ Z_{j-1})^T \mathbf{1} / (D_{j-1} \circ V_j)^T \mathbf{1}$ \\
$U_j\leftarrow U_{j-1} + \textrm{diag}(\balpha_j) D_{j-1}$\\
$R_j \leftarrow R_{j-1} - \textrm{diag}(\alpha_j) V_j$ \\
\textbf{if} $\forall i \quad \| r_j^{(i)}\|_2 < $ \textbf{then return} $U_j$ \;
$Z_i \leftarrow \hat{P}^{-1} (R_j)$ \\
$\bbeta_j \leftarrow (R_j \circ Z_j)^T \mathbf{1} / (R_{j-1} \circ Z_{j-1})^T \mathbf{1}$ \\
$D_j \leftarrow Z_j + \textrm{diag} (\beta_j) D_{j-1}$ \\
$\forall i \quad [\tilde{T}]_{j,j} \leftarrow  1/[\alpha_j]_i + [\beta_{j-1}]_i / [\alpha_{j-1}]_i$ \\
$\forall i \quad [\tilde{T}_i]_{j-1, j}, [\tilde{T}_i]_{j, j-1} \leftarrow \sqrt{[\beta_{j-1}]_i} / [\alpha_j]_i$
}
\Return{$U_{j+1}, \tilde{T}_1, \cdots, \tilde{T}_t$}
 %\While{While condition}{
%  instructions\;
%  \eIf{condition}{
%   instructions1\;
%   instructions2\;%
%   }{
%   instructions3\;
%  }
% }
 \caption{Modified Batch Conjugate Gradient (mBCG)}
 \label{alg:mbcg}
\end{algorithm}


\section{RR-CG}

For notational simplicity, we write $\bK = \hat{\bK}_{\bX \bX}$.
Denote the system size, i.e. the size of matrix $\bK$ by $N$.
\begin{enumerate}
  \item CG solution estimates.

  For $\bK^{-1} \by$, CG computes:
  \begin{align}
    \bK^{-1} \by &= \sum_{j=1}^N \gamma_j \bd_j
  \end{align}
  where $d_j$ are conjugate search directions.

  The RR estimate is:
  \begin{align}
    \bK^{-1} \by \approx \sum_{j=1}^J \frac{\gamma_j \bd_j}{1-F(j-1)}
  \end{align}
  where $J \sim P(J)$ and $F(\cdot)$ is the cumulative distribution function associated with $p(J)$.

  Similarly, we can compute the RR estimate for $\bK^{-1}\bz$.

  \textbf{Note:} for the backward pass computation
  \begin{align*}
    \by^T \bK^{-1} \frac{\partial \bK}{\partial \theta}  \bK^{-1} \by,
  \end{align*}
  the unbiased estimate is
  \begin{align}
    \widehat{\by^T \bK^{-1}}_1 \frac{\partial \bK}{\partial \theta}  \widehat{\bK^{-1} \by}_2,
  \end{align}
   where we use two independent RR estimates for the left linear-solve and the right linear-solve, i.e.
  \begin{align}
    \widehat{\by^T \bK^{-1}}_1 &= \sum_{j=1}^{J_1} \frac{\gamma_j \bd_j} {1-F_1 (j-1)}, \\
    \widehat{\by^T \bK^{-1}}_2 &= \sum_{j=1}^{J_2} \frac{\gamma_j \bd_j}{1-F_2 (j-1)}.
  \end{align}
   Here $J_1 \sim p_1(J)$, $J_2 \sim p_2(J)$, $J_1$ and $J_2$ are independent, and
  $F_1(\cdot)$ and $F_2(\cdot)$ are cdfs of the seperate distributions.

  \item logdet term computes:
  \begin{align}
    \log | \bK | &\approx \frac{1}{t} \sum_{i=1}^t [(\bV_i \be_1)^T (\log \bLambda_i ) (\bV_i \be_1)]
  \end{align}
  For notational simplicity,
  consider $i=1$ (the number of probe vectors is 1), so we drop the subscript $i$.
  We use the subscript $j$ to denote the size of $T$ matrix.
  That is, $\bT_j$ is the partial Lanczos tridiagonalization of $\bK$ after $j$ iterations,
  and $\bT_j = \bV_j \bLambda_j \bV_j^T$. Then
  \begin{align}
    \log | \bK | &\approx (\bV_N \be_1)^T (\log \bLambda_N ) (\bV_N \be_1) \\
    &= (\bV_1 \be_1)^T (\log \bLambda_1 ) (\bV_1 \be_1) + \\
   &\qquad   \sum_{j=2}^N \big[ (\bV_j \be_1)^T (\log \bLambda_j) (\bV_j \be_1) \\
   & \qquad \qquad - (\bV_{j-1} \be_1)^T (\log \bLambda_{j-1} ) (\bV_{j-1} \be_1) \big].
  \end{align}
  The RR estimate is:
  \begin{align}
    \log | \bK |  &\approx (\bV_1 \be_1)^T (\log \bLambda_1 ) (\bV_1 \be_1) + \\
   &\qquad   \sum_{j=2}^N \big[ (\bV_j \be_1)^T (\log \bLambda_j) (\bV_j \be_1) \\
   & \qquad \qquad - (\bV_{j-1} \be_1)^T (\log \bLambda_{j-1} ) (\bV_{j-1} \be_1) \big] / \left(1- F(j-1)\right).
  \end{align}
\end{enumerate}


\iffalse
\begin{algorithm}[H]
\SetAlgoLined
\KwInput{$mmm_A()$ -- function for matrix-matrix multiplication with $A$ \\
        $B$ -- $n\times t$ matrix to solve against \\
        $\hat{P}^{-1}$ -- func. for preconditioner\\} \\
        $F(\cdot)$ -- distribution for sampling termination number \\
\KwOutput{$A^{-1}B$, $\tilde{T}_1, \cdots, \tilde{T}_t$}
 %initialization
 $U_0 \leftarrow \mathbf{0}$ \gray{// Current solutions} \\
 $R_0 \leftarrow B - mmm_A(U_0)$ \gray{// Current residuals} \\
 $Z_0 \leftarrow P^{-1}(R_0)$ \gray{// Preconditioned residuals} \\
 $D_0 \leftarrow Z_0$ \gray{// Search directions for next solutions} \\
 $\tilde{T}_1, \cdots, \tilde{T}_t \leftarrow 0$ \gray{// Tridiag matrices} \\
\For{$j \leftarrow 0$ \KwTo $t$}{
$V_j \leftarrow mmm_A (D_{j-1})$ \\
$\balpha \leftarrow (R_{j-1} \circ Z_{j-1})^T \mathbf{1} / (D_{j-1} \circ V_j)^T \mathbf{1}$ \\
$U_j\leftarrow U_{j-1} + \textrm{diag}(\balpha_j) D_{j-1}$\\
$R_j \leftarrow R_{j-1} - \textrm{diag}(\alpha_j) V_j$ \\
\textbf{if} $\forall i \quad \| r_j^{(i)}\|_2 < $ \textbf{then return} $U_j$ \;
$Z_i \leftarrow \hat{P}^{-1} (R_j)$ \\
$\bbeta_j \leftarrow (R_j \circ Z_j)^T \mathbf{1} / (R_{j-1} \circ Z_{j-1})^T \mathbf{1}$ \\
$D_j \leftarrow Z_j + \textrm{diag} (\beta_j) D_{j-1}$ \\
$\forall i \quad [\tilde{T}]_{j,j} \leftarrow  1/[\alpha_j]_i + [\beta_{j-1}]_i / [\alpha_{j-1}]_i$ \\
$\forall i \quad [\tilde{T}_i]_{j-1, j}, [\tilde{T}_i]_{j, j-1} \leftarrow \sqrt{[\beta_{j-1}]_i} / [\alpha_j]_i$
}
\Return{$U_{j+1}, \tilde{T}_1, \cdots, \tilde{T}_t$}
 %\While{While condition}{
%  instructions\;
%  \eIf{condition}{
%   instructions1\;
%   instructions2\;%
%   }{
%   instructions3\;
%  }
% }
 \caption{Modified Batch Conjugate Gradient (mBCG)}
 \label{alg:mbcg}
\end{algorithm}
\fi

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliographystyle{apalike}

\bibliography{sample}

%----------------------------------------------------------------------------------------


\end{document}
